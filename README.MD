# LLM Examples for ARM Devices

## Best Local Model (ARM CPU)

ðŸ† **llama.cpp**

You're already using the right tool! ðŸš€

### Why llama.cpp wins on ARM

- âœ… Native aarch64 support
- âœ… Optimized kernels (NEON, DOTPROD)
- âœ… GGUF format (low RAM usage)
- âœ… Extremely stable performance
- âœ… Works on ARM servers, Macs, and Single Board Computers (SBCs)

### Features

âœ”ï¸ Chat / completions
âœ”ï¸ Coding capabilities
âœ”ï¸ JSON support (best-effort)
âœ”ï¸ Long context handling

### What it does NOT support

âŒ MCP (Model Control Protocol)
âŒ Tool / function calling
âŒ Full JSON Schema support

### Ideal Models for ARM

- Qwen3-4B-Instruct GGUF
- Ministral 3 3B
- Mistral 7B Q4/Q5
- Phi-3 Mini

> ðŸ’¡ Tip: llama.cpp is the optimal choice for ARM-based devices due to its native support, performance, and low memory requirements.